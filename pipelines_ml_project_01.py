# -*- coding: utf-8 -*-
"""Pipelines_ML_Project_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vdarUTUvzut-yv5bnwBtgjCkiSPS8mvm
"""

# Import necessary libraries
import pandas as pd  # Import pandas library for data manipulation
from sklearn.model_selection import train_test_split  # Import train_test_split for splitting the dataset
from sklearn.preprocessing import StandardScaler  # Import StandardScaler for feature scaling
from imblearn.over_sampling import SMOTE  # Import SMOTE for dataset balancing
from sklearn.linear_model import LogisticRegression  # Import LogisticRegression for logistic regression model
from xgboost import XGBClassifier  # Import XGBClassifier for XGBoost model
from sklearn.metrics import matthews_corrcoef, precision_score, recall_score, accuracy_score, f1_score, roc_auc_score  # Import evaluation metrics

# Load the dataset with headers from the first row
data = pd.read_csv('/content/dataset of Water.txt', header=0)

data.head()

data.describe().T

# Extract the feature names from the dataset
csv_feature_names = data.columns.tolist()  # Convert the column names to a list

csv_feature_names

# Now you can proceed with the rest of your code using the features

# Data Preprocessing
X = data.drop(columns=['Failure'])  # Features: Drop the target variable 'Failure'
y = data['Failure']  # Target variable: 'Failure'

X.head()

y.head()

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% training, 20% testing

# Identify non-numeric columns
non_numeric_columns = X_train.select_dtypes(include=['object']).columns  # Identify non-numeric columns

# Drop non-numeric columns before feature scaling
X_train_numeric = X_train.drop(columns=non_numeric_columns)  # Exclude non-numeric columns from training data
X_test_numeric = X_test.drop(columns=non_numeric_columns)  # Exclude non-numeric columns from test data

# Feature Scaling
scaler = StandardScaler()  # Initialize StandardScaler object
X_train_scaled = scaler.fit_transform(X_train_numeric)  # Fit and transform training data
X_test_scaled = scaler.transform(X_test_numeric)  # Transform test data using fitted scaler

# Dataset Balancing using SMOTE
smote = SMOTE(random_state=42)  # Initialize SMOTE object with a random state for reproducibility
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)  # Apply SMOTE to balance the training set

# Model Training and Evaluation
# Logistic Regression
logit_model = LogisticRegression()  # Initialize logistic regression model
logit_model.fit(X_train_balanced, y_train_balanced)  # Fit logistic regression model to balanced training data
logit_predictions = logit_model.predict(X_test_scaled)  # Make predictions on the test set using the logistic regression model

# XGBoost
xgb_model = XGBClassifier()  # Initialize XGBoost classifier
xgb_model.fit(X_train_balanced, y_train_balanced)  # Fit XGBoost model to balanced training data
xgb_predictions = xgb_model.predict(X_test_scaled)  # Make predictions on the test set using the XGBoost model

# Evaluation Metrics
# Calculate evaluation metrics for logistic regression predictions
logit_mcc = matthews_corrcoef(y_test, logit_predictions)
logit_precision = precision_score(y_test, logit_predictions)
logit_recall = recall_score(y_test, logit_predictions)
logit_accuracy = accuracy_score(y_test, logit_predictions)
logit_f1 = f1_score(y_test, logit_predictions)
logit_auc = roc_auc_score(y_test, logit_predictions)

# Calculate evaluation metrics for XGBoost predictions
xgb_mcc = matthews_corrcoef(y_test, xgb_predictions)
xgb_precision = precision_score(y_test, xgb_predictions)
xgb_recall = recall_score(y_test, xgb_predictions)
xgb_accuracy = accuracy_score(y_test, xgb_predictions)
xgb_f1 = f1_score(y_test, xgb_predictions)
xgb_auc = roc_auc_score(y_test, xgb_predictions)

# Print evaluation metrics for logistic regression
print("Logistic Regression Metrics:")
print("MCC:", logit_mcc)
print("Precision:", logit_precision)
print("Recall:", logit_recall)
print("Accuracy:", logit_accuracy)
print("F1 Score:", logit_f1)
print("AUC:", logit_auc)

# Print evaluation metrics for XGBoost
print("\nXGBoost Metrics:")
print("MCC:", xgb_mcc)
print("Precision:", xgb_precision)
print("Recall:", xgb_recall)
print("Accuracy:", xgb_accuracy)
print("F1 Score:", xgb_f1)
print("AUC:", xgb_auc)

